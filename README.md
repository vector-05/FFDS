# FFDS
Financial Fraud Detection System - Basic Project
This a simple project created to understand data analysis and data manipulation in python.
INTRODUCTION

The software created in this project aims to calculate the chances of a provided transaction or sheet of transactions (CSV file) of being fraudulent by making extensive use of data provided data and to then allow the visualization of the outputs produced. The software encompasses three core functionalities namely manipulation, analysis and visualization of data. The software functions by first reading a provided sheet of transaction records that consists of both fraudulent and non-fraudulent transactions that are randomly generated. Essential data for each field is calculated and stored for use. The software then asks the user whether an individual transaction (query) or a sheet of transactions (in the form of CSV file) need to be analyzed. Upon receiving the query, sheet or individual, the software stores it in the form of DataFrame and analyzes the query using the previously calculated data. For analysis, it rewards the query with a score that is logically determined. This score is divided into two parts – first part is awarded on basis on time parameters of the query and second part is awarded by categorizing the query on basis of its Transaction Amount and Account Balance proportion and awards the score on basis of frequency of frauds in the required category. This scores are different points that defined in the software and are awarded according to certain logical parameters. After calculation of score the software categorizes the query as either having critical chances of being fraudulent or mild. For an individual query the analysis is then sourced as output while for a sheet query visualization using graphs is provided as well. Moreover, sheet queries also use the third functionality of manipulation where the software allows updation, deletion or formation of data in provided sheet prior to its analysis. 

OBJECTIVE AND SCOPE OF PROJECT

The objective of the project is to develop a software that can make logical use of data which is extracted from provided records and then to logically use that data on provided transaction or sheet of transactions to analyze and visualize it and estimate the chances that a transaction has of being a fraudulent case.  
Moreover, the project is aimed to create a thorough understanding of software development and to develop the ability to design creative and logical code that can bring about the desired result from the software.

LOGIC USED
The goal of analysis of the software is to logically reward the query with a fraud score that determines the chances of that query of being a fraudulent transaction. This score is divided into two parts. The first part is time-based score and is denoted by ‘a’. This score is calculated by first figuring out the frequency of frauds in the time slot of user query (denoted as ‘d’) and then it is subdivided into two scores namely – ‘a1’ and ‘a2’. ‘a1’ is awarded by comparing the value of ‘d’ to the average fraud frequency in all time slots (denoted as ‘ave’). ‘a2’  is calculated on basis of how much the  value of ‘d’ deviated from ‘ave’. This is done by comparing standard deviation in all time slots (denoted as ‘st.d’) to standard deviation in user query time slot (denoted as ‘tr.d’) and is done in such a manner that if ‘d’ is greater than ‘ave’ than the order of awarding ‘a2’ id retained otherwise reversed so that the scoring remains consistent.
The second half of total score is based on the proportion of Transaction Amount and Account Balance (denoted as ‘TrA_AcBa Proportion’) and is denoted by ‘b’. It is further subdivided into two sections namely – ‘b1’ and ‘b2’. ‘b1’ is calculated by categorizing TrA_AcBa Proportion of user query in a defined proportion category and then comparing the frequencies of fraud in the category of the user query (referred as Normal Proximity Frequency or NPF) with the fraud frequencies in category above user query (referred as Upper Proximity Frequency or UPF) and below user query (referred as Lower Proximity Frequency or LPF) and then score is awarded by comparing the three frequencies in a defined manner. ‘b2’ is calculated in a similar manner but instead of considering the entire dataset it only considers the dataset of the Transaction Category of user query. All the transaction records consist of one category in which the random transaction took place. For the purpose of this project there are a total of four hypothetical categories called PRC 1, PRC 2, PRC 3 and PRC 4 (PRC stands for Project Restricted Category). The same procedure as ‘b1’ is followed but only in the required category.
The final score then calculated by adding all the four sub-scores and is given out of a score of 10 (All four sub-scores are 2.5 each). This is also converted into percentage. The transaction is then categorized as critical if it has more than 80% chance of being fraud or mild if the chances are between 30-80% respectively.

FILES AND SOURCE CODE
User_Interface.py : It is the main file of the software through which the user interacts and functions as a communicator between the user and the rest of the software
tempdata.py : When the software runs this file stores the previously provided CSV file and runs specific functions to extract valuable statistical data from it. This data is then outsourced to the data analysis files to be logically used to analyze the user query.
Time_Suspicion_Score.py: This file calculates the score ‘a’ to be awarded. It inputs statistical data from tempdata.py and user query from User_Interface.py. It is sub-divided into two functions which award score to individual and sheet query respectively. This data is then sent to the User_Interface.py file for further analysis
TrA_AcBa_Joint_Suspicion_Score.py : This file calculates the b1 score. This frequency data are statistical data inputted from tempdata.py. It also has two different functions for individual and sheet query respectively. It then accordingly calculates the score which is then outsourced to the User_Interface.py file for further analysis
TrA_AcBa_Joint_Suspicion_Scoreb2.py : This file calculates the second part ‘b2’ of the ‘b’ score. The fraud frequency data and user query are insourced from tempdata.py and User_Interface.py respectively and analyzed data is outsourced to User_Interface.py for further analysis
Data_analysis.py: In case of a sheet query, all the analyzed data that the User_Interface.py file receives from the prior three files is then sent to this file which sorts the data into desired categories (critical or mild) and finalizes a report for the query sheet including graphs. This final data is then outsourced to User_Interface.py which outsources it to the user
Data_manipulation.py: Apart from analysis and visualization another functionality that the software provides to sheet query is manipulation of data in the provided file. This functionality is solely conducted by this file which both insources and outsources its data from User_Interface.py file
Data_generator.py: This file is used to produce random sheets of transaction records for both data feed and user query purposes.
